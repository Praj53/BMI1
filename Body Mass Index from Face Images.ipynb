{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in e:\\abc\\lib\\site-packages (4.8.1.78)\n",
      "Requirement already satisfied: numpy>=1.19.3 in e:\\abc\\lib\\site-packages (from opencv-python) (1.26.3)\n"
     ]
    }
   ],
   "source": [
    "! pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft Windows [Version 10.0.19045.3208]\r\n",
      "(c) Microsoft Corporation. All rights reserved.\r\n",
      "\r\n",
      "C:\\Users\\LENOVO L460\\Downloads\\Body-Mass-Index-from-Face-Images-main\\Body-Mass-Index-from-Face-Images-main>Where python\n",
      "E:\\abc\\python.exe\r\n",
      "C:\\Users\\LENOVO L460\\AppData\\Local\\Programs\\Python\\Python39\\python.exe\r\n",
      "C:\\Users\\LENOVO L460\\AppData\\Local\\Microsoft\\WindowsApps\\python.exe\r\n",
      "\r\n",
      "C:\\Users\\LENOVO L460\\Downloads\\Body-Mass-Index-from-Face-Images-main\\Body-Mass-Index-from-Face-Images-main>"
     ]
    }
   ],
   "source": [
    "%%cmd \n",
    "Where python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft Windows [Version 10.0.19045.3208]\r\n",
      "(c) Microsoft Corporation. All rights reserved.\r\n",
      "\r\n",
      "C:\\Users\\LENOVO L460\\Downloads\\Body-Mass-Index-from-Face-Images-main\\Body-Mass-Index-from-Face-Images-main>pip install face-recognition\n",
      "Requirement already satisfied: face-recognition in e:\\abc\\lib\\site-packages (1.3.0)\r\n",
      "Requirement already satisfied: numpy in e:\\abc\\lib\\site-packages (from face-recognition) (1.26.3)\r\n",
      "Requirement already satisfied: Pillow in e:\\abc\\lib\\site-packages (from face-recognition) (9.2.0)\r\n",
      "Requirement already satisfied: dlib>=19.7 in e:\\abc\\lib\\site-packages (from face-recognition) (19.22.99)\r\n",
      "Requirement already satisfied: Click>=6.0 in e:\\abc\\lib\\site-packages (from face-recognition) (8.0.4)\r\n",
      "Requirement already satisfied: face-recognition-models>=0.3.0 in e:\\abc\\lib\\site-packages (from face-recognition) (0.3.0)\r\n",
      "Requirement already satisfied: colorama in e:\\abc\\lib\\site-packages (from Click>=6.0->face-recognition) (0.4.6)\r\n",
      "\r\n",
      "C:\\Users\\LENOVO L460\\Downloads\\Body-Mass-Index-from-Face-Images-main\\Body-Mass-Index-from-Face-Images-main>"
     ]
    }
   ],
   "source": [
    "%%cmd\n",
    "pip install face-recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft Windows [Version 10.0.19045.3208]\r\n",
      "(c) Microsoft Corporation. All rights reserved.\r\n",
      "\r\n",
      "C:\\Users\\LENOVO L460\\Downloads\\Body-Mass-Index-from-Face-Images-main\\Body-Mass-Index-from-Face-Images-main>pip install cmake\n",
      "Requirement already satisfied: cmake in e:\\abc\\lib\\site-packages (3.28.1)\r\n",
      "\r\n",
      "C:\\Users\\LENOVO L460\\Downloads\\Body-Mass-Index-from-Face-Images-main\\Body-Mass-Index-from-Face-Images-main>"
     ]
    }
   ],
   "source": [
    "%%cmd \n",
    "pip install cmake\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "geEirVAGRJLK"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\abc\\lib\\site-packages\\scipy\\__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\abc\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft Windows [Version 10.0.19045.3208]\r\n",
      "(c) Microsoft Corporation. All rights reserved.\r\n",
      "\r\n",
      "C:\\Users\\LENOVO L460\\Downloads\\Body-Mass-Index-from-Face-Images-main\\Body-Mass-Index-from-Face-Images-main>python --version\n",
      "Python 3.9.13\r\n",
      "\r\n",
      "C:\\Users\\LENOVO L460\\Downloads\\Body-Mass-Index-from-Face-Images-main\\Body-Mass-Index-from-Face-Images-main>"
     ]
    }
   ],
   "source": [
    "%%cmd\n",
    "python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft Windows [Version 10.0.19045.3208]\r\n",
      "(c) Microsoft Corporation. All rights reserved.\r\n",
      "\r\n",
      "C:\\Users\\LENOVO L460\\Downloads\\Body-Mass-Index-from-Face-Images-main\\Body-Mass-Index-from-Face-Images-main>pip install \"C:\\Users\\LENOVO L460\\Desktop\\dlib\\dlib-19.22.99-cp39-cp39-win_amd64.whl\"\n",
      "Processing c:\\users\\lenovo l460\\desktop\\dlib\\dlib-19.22.99-cp39-cp39-win_amd64.whl\r\n",
      "\r\n",
      "C:\\Users\\LENOVO L460\\Downloads\\Body-Mass-Index-from-Face-Images-main\\Body-Mass-Index-from-Face-Images-main>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Requirement 'C:\\\\Users\\\\LENOVO L460\\\\Desktop\\\\dlib\\\\dlib-19.22.99-cp39-cp39-win_amd64.whl' looks like a filename, but the file does not exist\r\n",
      "ERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\LENOVO L460\\\\Desktop\\\\dlib\\\\dlib-19.22.99-cp39-cp39-win_amd64.whl'\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "%%cmd\n",
    "pip install \"C:\\Users\\LENOVO L460\\Desktop\\dlib\\dlib-19.22.99-cp39-cp39-win_amd64.whl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft Windows [Version 10.0.19045.3208]\r\n",
      "(c) Microsoft Corporation. All rights reserved.\r\n",
      "\r\n",
      "C:\\Users\\LENOVO L460\\Downloads\\Body-Mass-Index-from-Face-Images-main\\Body-Mass-Index-from-Face-Images-main>pip install face-recognition\n",
      "Requirement already satisfied: face-recognition in e:\\abc\\lib\\site-packages (1.3.0)\r\n",
      "Requirement already satisfied: Pillow in e:\\abc\\lib\\site-packages (from face-recognition) (9.2.0)\r\n",
      "Requirement already satisfied: face-recognition-models>=0.3.0 in e:\\abc\\lib\\site-packages (from face-recognition) (0.3.0)\r\n",
      "Requirement already satisfied: Click>=6.0 in e:\\abc\\lib\\site-packages (from face-recognition) (8.0.4)\r\n",
      "Requirement already satisfied: dlib>=19.7 in e:\\abc\\lib\\site-packages (from face-recognition) (19.22.99)\r\n",
      "Requirement already satisfied: numpy in e:\\abc\\lib\\site-packages (from face-recognition) (1.26.3)\r\n",
      "Requirement already satisfied: colorama in e:\\abc\\lib\\site-packages (from Click>=6.0->face-recognition) (0.4.6)\r\n",
      "\r\n",
      "C:\\Users\\LENOVO L460\\Downloads\\Body-Mass-Index-from-Face-Images-main\\Body-Mass-Index-from-Face-Images-main>"
     ]
    }
   ],
   "source": [
    "%%cmd \n",
    "pip install face-recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cassandra-driver in e:\\abc\\lib\\site-packages (3.29.0)\n",
      "Requirement already satisfied: geomet<0.3,>=0.1 in e:\\abc\\lib\\site-packages (from cassandra-driver) (0.2.1.post1)\n",
      "Requirement already satisfied: click in e:\\abc\\lib\\site-packages (from geomet<0.3,>=0.1->cassandra-driver) (8.0.4)\n",
      "Requirement already satisfied: six in e:\\abc\\lib\\site-packages (from geomet<0.3,>=0.1->cassandra-driver) (1.16.0)\n",
      "Requirement already satisfied: colorama in e:\\abc\\lib\\site-packages (from click->geomet<0.3,>=0.1->cassandra-driver) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install cassandra-driver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cassandra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "8BPUOfO0T1Fk"
   },
   "outputs": [],
   "source": [
    "import face_recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Pandas to read the CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "EB7GZ8pNRJLo"
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"bmi data set.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating the log file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(filename='bmi.log', level=logging.INFO, format='%(asctime)s:%(levelname)s:%(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Aps4wTSZRJLq"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path as p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Q-CAVjkJRJLr"
   },
   "outputs": [],
   "source": [
    "#Identifying the image name and commencing iteration from 0\n",
    "\n",
    "def get_index_of_digit(string):\n",
    "    import re\n",
    "    match = re.search(\"\\d\", p(string).stem)\n",
    "    return match.start(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifying the data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "J_f8Dtj7RJLt"
   },
   "outputs": [],
   "source": [
    "data_folder = \"sample_faces\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pfHKwoeORJMq",
    "outputId": "3a3f235f-0346-4751-f6b2-c914d06fa8ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 257 photos \n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "all_files = glob(data_folder+\"/*\")\n",
    "\n",
    "all_jpgs = sorted([img for img in all_files if \".jpg\" in img or \".jpeg\" in img or \"JPG\" or \"png\" or \"PNG\" in img])\n",
    "logging.info(\"Total %d photos\", len(all_jpgs))\n",
    "print(\"Total {} photos \".format(len(all_jpgs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "K4NFOaQrRJMu"
   },
   "outputs": [],
   "source": [
    "id_dir = [(p(images).stem[:(get_index_of_digit(p(images).stem))],images) for  images in all_jpgs ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "1sTj4ai8RJMw"
   },
   "outputs": [],
   "source": [
    "image_df = pd.DataFrame(id_dir,columns=['UID','path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "SLok4X9fRJMz"
   },
   "outputs": [],
   "source": [
    "data_df = image_df.merge(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "MgPJxaHARJM0",
    "outputId": "e95a5f3f-46e5-408f-89ba-0f7c52f20f77"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UID</th>\n",
       "      <th>path</th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>akshay</td>\n",
       "      <td>sample_faces\\akshay1.jpeg</td>\n",
       "      <td>1</td>\n",
       "      <td>akshay kumar</td>\n",
       "      <td>1.78</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>akshay</td>\n",
       "      <td>sample_faces\\akshay10.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>akshay kumar</td>\n",
       "      <td>1.78</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>akshay</td>\n",
       "      <td>sample_faces\\akshay11.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>akshay kumar</td>\n",
       "      <td>1.78</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>akshay</td>\n",
       "      <td>sample_faces\\akshay12.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>akshay kumar</td>\n",
       "      <td>1.78</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>akshay</td>\n",
       "      <td>sample_faces\\akshay13.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>akshay kumar</td>\n",
       "      <td>1.78</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>vimal</td>\n",
       "      <td>sample_faces\\vimal1.jpg</td>\n",
       "      <td>45</td>\n",
       "      <td>vimal</td>\n",
       "      <td>1.75</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>vishal</td>\n",
       "      <td>sample_faces\\vishal1.jpg</td>\n",
       "      <td>35</td>\n",
       "      <td>vishal</td>\n",
       "      <td>1.80</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>vishnuvishal</td>\n",
       "      <td>sample_faces\\vishnuvishal1.jpg</td>\n",
       "      <td>46</td>\n",
       "      <td>vishnuvishal</td>\n",
       "      <td>1.77</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>vjantony</td>\n",
       "      <td>sample_faces\\vjantony1.jpg</td>\n",
       "      <td>49</td>\n",
       "      <td>vjantony</td>\n",
       "      <td>1.75</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>vjsethu</td>\n",
       "      <td>sample_faces\\vjsethu1.jpg</td>\n",
       "      <td>48</td>\n",
       "      <td>vjsethu</td>\n",
       "      <td>1.77</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>257 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              UID                            path  id          name  height  \\\n",
       "0          akshay       sample_faces\\akshay1.jpeg   1  akshay kumar    1.78   \n",
       "1          akshay       sample_faces\\akshay10.jpg   1  akshay kumar    1.78   \n",
       "2          akshay       sample_faces\\akshay11.jpg   1  akshay kumar    1.78   \n",
       "3          akshay       sample_faces\\akshay12.jpg   1  akshay kumar    1.78   \n",
       "4          akshay       sample_faces\\akshay13.jpg   1  akshay kumar    1.78   \n",
       "..            ...                             ...  ..           ...     ...   \n",
       "252         vimal         sample_faces\\vimal1.jpg  45         vimal    1.75   \n",
       "253        vishal        sample_faces\\vishal1.jpg  35        vishal    1.80   \n",
       "254  vishnuvishal  sample_faces\\vishnuvishal1.jpg  46  vishnuvishal    1.77   \n",
       "255      vjantony      sample_faces\\vjantony1.jpg  49      vjantony    1.75   \n",
       "256       vjsethu       sample_faces\\vjsethu1.jpg  48       vjsethu    1.77   \n",
       "\n",
       "     weight  \n",
       "0        80  \n",
       "1        80  \n",
       "2        80  \n",
       "3        80  \n",
       "4        80  \n",
       "..      ...  \n",
       "252      69  \n",
       "253      73  \n",
       "254      73  \n",
       "255      67  \n",
       "256      84  \n",
       "\n",
       "[257 rows x 6 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the function for extracting face data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "WnvejAv5RJM1"
   },
   "outputs": [],
   "source": [
    "def my_face_encoding(image_path):\n",
    "    print(image_path)\n",
    "    logging.info(\"Getting face encoding for image %s\", image_path)\n",
    "    picture_of_me = face_recognition.load_image_file(image_path)\n",
    "    my_face_encoding = face_recognition.face_encodings(picture_of_me)\n",
    "    if not my_face_encoding:\n",
    "        print(\"no face found !!!\")\n",
    "        logging.warning(\"No face found in image %s\", image_path)\n",
    "        return np.zeros(128).tolist()\n",
    "    return my_face_encoding[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "-d2GTl9DRJM3"
   },
   "outputs": [],
   "source": [
    "tot_faces = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iteration and appending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ugSwX83rRJM8",
    "outputId": "9dc8d7bd-707f-4208-f8d1-52771b15a621"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_faces\\akshay1.jpeg\n",
      "sample_faces\\akshay10.jpg\n",
      "sample_faces\\akshay11.jpg\n",
      "sample_faces\\akshay12.jpg\n",
      "sample_faces\\akshay13.jpg\n",
      "sample_faces\\akshay14.jpg\n",
      "sample_faces\\akshay15.jpg\n",
      "sample_faces\\akshay16.jpg\n",
      "sample_faces\\akshay17.jpg\n",
      "sample_faces\\akshay18.jpg\n",
      "sample_faces\\akshay19.jpg\n",
      "sample_faces\\akshay2.jpeg\n",
      "no face found !!!\n",
      "sample_faces\\akshay20.jpg\n",
      "sample_faces\\akshay3.jpg\n",
      "sample_faces\\akshay4.jpg\n",
      "sample_faces\\akshay5.jpg\n",
      "sample_faces\\akshay6.jpg\n",
      "sample_faces\\akshay7.jpg\n",
      "sample_faces\\akshay8.jpg\n",
      "sample_faces\\akshay9.jpg\n",
      "sample_faces\\amir1.jpg\n",
      "sample_faces\\amir10.jpg\n",
      "sample_faces\\amir11.jpg\n",
      "sample_faces\\amir12.jpg\n",
      "sample_faces\\amir13.jpg\n",
      "sample_faces\\amir14.jpeg\n",
      "sample_faces\\amir15.jpg\n",
      "sample_faces\\amir2.jpg\n",
      "sample_faces\\amir3.jpg\n",
      "sample_faces\\amir4.jpeg\n",
      "no face found !!!\n",
      "sample_faces\\amir5.jpg\n",
      "sample_faces\\amir6.PNG\n",
      "sample_faces\\amir7.jpeg\n",
      "sample_faces\\amir8.jpg\n",
      "sample_faces\\amir9.jpg\n",
      "sample_faces\\anupam2.jpg\n",
      "sample_faces\\anupam3.jpg\n",
      "sample_faces\\anupam4.jpg\n",
      "sample_faces\\anupam5.jpg\n",
      "sample_faces\\anupam6.jpg\n",
      "sample_faces\\anupam7.jpeg\n",
      "sample_faces\\anupam8.jpg\n",
      "sample_faces\\anurag1.jpg\n",
      "sample_faces\\anurag2.jpg\n",
      "sample_faces\\anurag3.jpg\n",
      "sample_faces\\anurag4.jpg\n",
      "sample_faces\\anurag5.jpg\n",
      "sample_faces\\arshad1.jpg\n",
      "sample_faces\\arshad10.jpg\n",
      "sample_faces\\arshad11.jpg\n",
      "sample_faces\\arshad12.jpeg\n",
      "sample_faces\\arshad13.jpg\n",
      "sample_faces\\arshad14.jpg\n",
      "sample_faces\\arshad15.jpg\n",
      "sample_faces\\arshad16.jpg\n",
      "sample_faces\\arshad2.jpg\n",
      "sample_faces\\arshad3.jpeg\n",
      "sample_faces\\arshad4.jpg\n",
      "sample_faces\\arshad5.jpg\n",
      "sample_faces\\arshad6.jpg\n",
      "sample_faces\\arshad7.jpeg\n",
      "sample_faces\\arshad8.jpeg\n",
      "sample_faces\\arshad9.jpg\n",
      "sample_faces\\arya1.jpg\n",
      "sample_faces\\ayushman1.jpg\n",
      "sample_faces\\ayushman2.jpeg\n",
      "sample_faces\\ayushman3.JPG\n",
      "sample_faces\\ayushman4.jpg\n",
      "sample_faces\\ayushman5.jpg\n",
      "sample_faces\\bharath1.jpg\n",
      "sample_faces\\dhanush1.jpg\n",
      "sample_faces\\harishkalyan1.jpg\n",
      "sample_faces\\ja1.jpg\n",
      "sample_faces\\ja10.jpg\n",
      "sample_faces\\ja11.jpg\n",
      "sample_faces\\ja12.jpg\n",
      "sample_faces\\ja13.jpeg\n",
      "sample_faces\\ja14.jpg\n",
      "sample_faces\\ja15.jpg\n",
      "sample_faces\\ja2.jpg\n",
      "sample_faces\\ja3.jpg\n",
      "sample_faces\\ja4.jpg\n",
      "sample_faces\\ja5.jpg\n",
      "sample_faces\\ja6.jpg\n",
      "sample_faces\\ja7.jpg\n",
      "sample_faces\\ja8.jpg\n",
      "sample_faces\\ja9.jpg\n",
      "sample_faces\\jayamravi1.jpg\n",
      "sample_faces\\jeevan1.jpg\n",
      "sample_faces\\kalki1.jpg\n",
      "sample_faces\\kalki10.jpg\n",
      "sample_faces\\kalki11.jpg\n",
      "sample_faces\\kalki12.jpg\n",
      "sample_faces\\kalki13.jpeg\n",
      "sample_faces\\kalki14.jpg\n",
      "sample_faces\\kalki15.jpg\n",
      "sample_faces\\kalki2.jpg\n",
      "sample_faces\\kalki3.jpeg\n",
      "sample_faces\\kalki4.jpg\n",
      "sample_faces\\kalki5.jpg\n",
      "sample_faces\\kalki6.jpg\n",
      "sample_faces\\kalki7.jpg\n",
      "sample_faces\\kalki8.jpg\n",
      "sample_faces\\kalki9.jpg\n",
      "sample_faces\\kirron1.jpg\n",
      "sample_faces\\kirron10.jpg\n",
      "sample_faces\\kirron2.jpg\n",
      "sample_faces\\kirron3.jpg\n",
      "sample_faces\\kirron4.jpg\n",
      "sample_faces\\kirron5.jpg\n",
      "sample_faces\\kirron6.jpg\n",
      "sample_faces\\kirron7.jpg\n",
      "sample_faces\\kirron8.jpg\n",
      "sample_faces\\kirron9.jpeg\n",
      "sample_faces\\krishna1.jpg\n",
      "sample_faces\\lawrence1.jpg\n",
      "sample_faces\\madhavan1.jpg\n",
      "sample_faces\\manoj1.jpg\n",
      "sample_faces\\manoj2.jpeg\n",
      "sample_faces\\manoj3.jpg\n",
      "sample_faces\\manoj4.jpg\n",
      "sample_faces\\manoj5.JPG\n",
      "sample_faces\\nakul1.jpg\n",
      "sample_faces\\nandha1.jpg\n",
      "sample_faces\\naraian1.jpg\n",
      "sample_faces\\navdeep1.jpg\n",
      "sample_faces\\nawaz1.jpg\n",
      "sample_faces\\nawaz2.jpg\n",
      "sample_faces\\nawaz3.jpg\n",
      "sample_faces\\nawaz4.jpg\n",
      "sample_faces\\nawaz5.png\n",
      "sample_faces\\pankaj1.jpg\n",
      "sample_faces\\pankaj2.jpg\n",
      "sample_faces\\pankaj3.jpg\n",
      "sample_faces\\pankaj5.jpg\n",
      "sample_faces\\pankaj6.jpg\n",
      "sample_faces\\pritiviraj1.jpg\n",
      "sample_faces\\radhika1.jpg\n",
      "sample_faces\\radhika10.jpg\n",
      "sample_faces\\radhika11.jpg\n",
      "sample_faces\\radhika12.jpg\n",
      "sample_faces\\radhika13.jpg\n",
      "sample_faces\\radhika14.jpg\n",
      "sample_faces\\radhika15.jpg\n",
      "sample_faces\\radhika2.jpeg\n",
      "sample_faces\\radhika3.jpg\n",
      "sample_faces\\radhika4.jpg\n",
      "sample_faces\\radhika5.jpg\n",
      "sample_faces\\radhika6.jpg\n",
      "sample_faces\\radhika7.jpg\n",
      "sample_faces\\radhika8.jpg\n",
      "sample_faces\\radhika9.jpg\n",
      "sample_faces\\rajkumar1.jpg\n",
      "sample_faces\\rajkumar2.jpg\n",
      "sample_faces\\rajkumar3.jpg\n",
      "sample_faces\\rajkumar4.jpg\n",
      "sample_faces\\rajkumar5.jpg\n",
      "sample_faces\\ratna1.jpeg\n",
      "sample_faces\\ratna2.jpg\n",
      "sample_faces\\ratna3.jpg\n",
      "sample_faces\\ratna4.JPG\n",
      "sample_faces\\ratna5.jpg\n",
      "sample_faces\\ratna6.jpg\n",
      "sample_faces\\richa2.jpg\n",
      "sample_faces\\richa3.JPG\n",
      "sample_faces\\richa4.jpg\n",
      "sample_faces\\richa5.jpg\n",
      "sample_faces\\rjbalaji1.jpg\n",
      "sample_faces\\salman1.jpg\n",
      "sample_faces\\salman10.jpg\n",
      "sample_faces\\salman11.jpg\n",
      "sample_faces\\salman12.jpg\n",
      "sample_faces\\salman13.jpg\n",
      "sample_faces\\salman14.jpg\n",
      "sample_faces\\salman15.jpg\n",
      "sample_faces\\salman16.jpg\n",
      "sample_faces\\salman17.jpg\n",
      "sample_faces\\salman18.jpg\n",
      "sample_faces\\salman19.jpg\n",
      "sample_faces\\salman2.jpg\n",
      "sample_faces\\salman20.jpg\n",
      "sample_faces\\salman3.jpg\n",
      "sample_faces\\salman4.jpg\n",
      "sample_faces\\salman5.jpeg\n",
      "sample_faces\\salman6.jpg\n",
      "sample_faces\\salman7.jpg\n",
      "sample_faces\\salman8.jpg\n",
      "sample_faces\\salman9.jpg\n",
      "sample_faces\\samuthirakani1.jpg\n",
      "sample_faces\\sandhanam1.jpg\n",
      "sample_faces\\siddharth1.jpg\n",
      "sample_faces\\simbu1.jpg\n",
      "sample_faces\\sjsurya1.jpg\n",
      "sample_faces\\srikanth1.jpg\n",
      "sample_faces\\srk1.jpg\n",
      "sample_faces\\srk10.jpeg\n",
      "sample_faces\\srk11.jpg\n",
      "sample_faces\\srk12.jpg\n",
      "sample_faces\\srk13.jpg\n",
      "sample_faces\\srk14.jpg\n",
      "sample_faces\\srk15.jpg\n",
      "sample_faces\\srk16.jpg\n",
      "sample_faces\\srk17.jpg\n",
      "sample_faces\\srk18.jpg\n",
      "sample_faces\\srk19.jpg\n",
      "sample_faces\\srk2.jpg\n",
      "sample_faces\\srk3.jpeg\n",
      "no face found !!!\n",
      "sample_faces\\srk4.jpg\n",
      "sample_faces\\srk5.jpg\n",
      "sample_faces\\srk6.jpg\n",
      "sample_faces\\srk7.jpg\n",
      "sample_faces\\srk8.jpg\n",
      "sample_faces\\srk9.jpg\n",
      "sample_faces\\supriya1.jpg\n",
      "sample_faces\\supriya2.jpg\n",
      "sample_faces\\supriya3.jpg\n",
      "sample_faces\\supriya4.jpg\n",
      "sample_faces\\supriya5.jpg\n",
      "sample_faces\\supriya6.jpg\n",
      "sample_faces\\tiger1.jpg\n",
      "sample_faces\\tiger2.jpg\n",
      "sample_faces\\tiger4.jpg\n",
      "sample_faces\\tiger5.jpg\n",
      "sample_faces\\tiger6.jpg\n",
      "sample_faces\\varun1.jpg\n",
      "sample_faces\\varun2.jpg\n",
      "sample_faces\\varun3.jpg\n",
      "sample_faces\\varun4.jpeg\n",
      "sample_faces\\varun5.png\n",
      "sample_faces\\varun6.jpeg\n",
      "sample_faces\\vijay1.jpg\n",
      "sample_faces\\vikky1.jpg\n",
      "sample_faces\\vikky10.jpg\n",
      "sample_faces\\vikky11.jpg\n",
      "sample_faces\\vikky12.jpg\n",
      "sample_faces\\vikky13.jpg\n",
      "sample_faces\\vikky14.jpg\n",
      "sample_faces\\vikky15.jpg\n",
      "sample_faces\\vikky16.jpg\n",
      "sample_faces\\vikky17.jpg\n",
      "sample_faces\\vikky18.jpg\n",
      "sample_faces\\vikky19.jpg\n",
      "sample_faces\\vikky2.jpg\n",
      "sample_faces\\vikky20.jpg\n",
      "sample_faces\\vikky3.jpg\n",
      "sample_faces\\vikky4.jpg\n",
      "sample_faces\\vikky5.jpg\n",
      "sample_faces\\vikky6.jpg\n",
      "sample_faces\\vikky7.jpg\n",
      "sample_faces\\vikky8.jpg\n",
      "sample_faces\\vikky9.jpg\n",
      "sample_faces\\vikranth1.jpg\n",
      "sample_faces\\vimal1.jpg\n",
      "sample_faces\\vishal1.jpg\n",
      "sample_faces\\vishnuvishal1.jpg\n",
      "sample_faces\\vjantony1.jpg\n",
      "sample_faces\\vjsethu1.jpg\n"
     ]
    }
   ],
   "source": [
    "for images in data_df.path:\n",
    "    face_enc = my_face_encoding(images)\n",
    "    tot_faces.append(face_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "J_RJ8zMURJM9"
   },
   "outputs": [],
   "source": [
    "X = np.array(tot_faces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "ErowbAzJRJM-"
   },
   "outputs": [],
   "source": [
    "y_height = data_df.height.values\n",
    "y_weight = data_df.weight.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "splitting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "AJivJdslRJND"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "d2v1xmWbRJNF"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_height_train, y_height_test, y_weight_train, y_weight_test = train_test_split(X, y_height,y_weight, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u9olli-IRJNH"
   },
   "source": [
    "Converting the data Type into float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "1sPyMGDQRJNP"
   },
   "outputs": [],
   "source": [
    "y_height_train=y_height_train.astype(float)\n",
    "y_height_test=y_height_test.astype(float)\n",
    "y_weight_train=y_weight_train.astype(float)\n",
    "y_weight_test=y_weight_test.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ZaZO_J4RJNU"
   },
   "source": [
    "Obtaining the shape of the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T97noavQRJNV",
    "outputId": "7eec7dae-5f0f-4456-e90b-93ca0475d941"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(192, 128)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dw61Oqo0RJNY"
   },
   "source": [
    "Training the model with CNN (Convolutional Neural Network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ScfSGjZnRJNZ"
   },
   "source": [
    "Developing height model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1dB8VNPvRJNZ",
    "outputId": "0bc726d4-2943-4293-b120-d5f65ba7cb92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\abc\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From E:\\abc\\lib\\site-packages\\keras\\src\\backend.py:6642: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From E:\\abc\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/13\n",
      "WARNING:tensorflow:From E:\\abc\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From E:\\abc\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "192/192 [==============================] - 3s 8ms/step - loss: 0.0063 - mae: 0.0502\n",
      "Epoch 2/13\n",
      "192/192 [==============================] - 2s 8ms/step - loss: 0.0015 - mae: 0.0296\n",
      "Epoch 3/13\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 9.7661e-04 - mae: 0.0238\n",
      "Epoch 4/13\n",
      "192/192 [==============================] - 2s 8ms/step - loss: 7.8666e-04 - mae: 0.0216\n",
      "Epoch 5/13\n",
      "192/192 [==============================] - 2s 8ms/step - loss: 6.7190e-04 - mae: 0.0208\n",
      "Epoch 6/13\n",
      "192/192 [==============================] - 2s 8ms/step - loss: 4.7112e-04 - mae: 0.0176\n",
      "Epoch 7/13\n",
      "192/192 [==============================] - 2s 8ms/step - loss: 7.5919e-04 - mae: 0.0216\n",
      "Epoch 8/13\n",
      "192/192 [==============================] - 2s 8ms/step - loss: 4.1614e-04 - mae: 0.0162\n",
      "Epoch 9/13\n",
      "192/192 [==============================] - 2s 8ms/step - loss: 3.5858e-04 - mae: 0.0154\n",
      "Epoch 10/13\n",
      "192/192 [==============================] - 2s 8ms/step - loss: 4.9173e-04 - mae: 0.0177\n",
      "Epoch 11/13\n",
      "192/192 [==============================] - 2s 8ms/step - loss: 7.1522e-04 - mae: 0.0212\n",
      "Epoch 12/13\n",
      "192/192 [==============================] - 2s 9ms/step - loss: 4.4880e-04 - mae: 0.0162\n",
      "Epoch 13/13\n",
      "192/192 [==============================] - 2s 8ms/step - loss: 4.2688e-04 - mae: 0.0164\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 5.2628e-04 - mae: 0.0178\n",
      "loss of height: 0.0005262807244434953\n",
      "mae of height: 0.017786609008908272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\abc\\lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# Define the model architecture\n",
    "model_height = tf.keras.Sequential([\n",
    "    tf.keras.layers.Reshape((X_train.shape[1], 1), input_shape=(X_train.shape[1],)),\n",
    "    tf.keras.layers.Conv1D(filters=32, kernel_size=3, activation='relu'),\n",
    "    tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "    tf.keras.layers.Conv1D(filters=64, kernel_size=3, activation='relu'),\n",
    "    tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "    tf.keras.layers.Conv1D(filters=128, kernel_size=3, activation='relu'),\n",
    "    tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(units=128, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=1, activation='relu')\n",
    "    ])\n",
    "model_height.compile(loss='mse', optimizer='adam',metrics=['mae'])\n",
    "\n",
    "# Fit the model to the training data\n",
    "model_height.fit(X_train, np.log(y_height_train), epochs=13, batch_size=1)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "test_loss_height_cnn,test_height_accuracy = model_height.evaluate(X_test, np.log(y_height_test))\n",
    "\n",
    "#logging \n",
    "logging.info(\"model height loss : %f, mae : %f\", test_loss_height_cnn,test_height_accuracy)\n",
    "\n",
    "#printing the metrics\n",
    "print('loss of height:', test_loss_height_cnn)\n",
    "print('mae of height:', test_height_accuracy)\n",
    "\n",
    "#saving the model\n",
    "model_height.save('height_model.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0v16IVyrRJNc"
   },
   "source": [
    "Developing weight model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D05eeD6KgWMw",
    "outputId": "7548fd97-7a89-44de-b6bb-61e9186fbc4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/13\n",
      "192/192 [==============================] - 3s 5ms/step - loss: 6.5964 - mae: 1.7441\n",
      "Epoch 2/13\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 0.0795 - mae: 0.1932\n",
      "Epoch 3/13\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 0.0695 - mae: 0.1905\n",
      "Epoch 4/13\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 0.0470 - mae: 0.1508\n",
      "Epoch 5/13\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 0.0493 - mae: 0.1679\n",
      "Epoch 6/13\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 0.0486 - mae: 0.1716\n",
      "Epoch 7/13\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 0.0270 - mae: 0.1240\n",
      "Epoch 8/13\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 0.0258 - mae: 0.1175\n",
      "Epoch 9/13\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 0.0172 - mae: 0.0996\n",
      "Epoch 10/13\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 0.0169 - mae: 0.1020\n",
      "Epoch 11/13\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 0.0204 - mae: 0.1156\n",
      "Epoch 12/13\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 0.0147 - mae: 0.0912\n",
      "Epoch 13/13\n",
      "192/192 [==============================] - 1s 5ms/step - loss: 0.0105 - mae: 0.0840\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0069 - mae: 0.0649\n",
      "loss of weight: 0.006913198623806238\n",
      "mae of weight: 0.06485950946807861\n"
     ]
    }
   ],
   "source": [
    "model_weight = tf.keras.Sequential([\n",
    "    tf.keras.layers.Reshape((X_train.shape[1], 1), input_shape=(X_train.shape[1],)),\n",
    "    tf.keras.layers.Conv1D(filters=32, kernel_size=3, activation='relu'),\n",
    "    tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "    tf.keras.layers.Conv1D(filters=64, kernel_size=3, activation='relu'),\n",
    "    tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "    tf.keras.layers.Conv1D(filters=128, kernel_size=3, activation='relu'),\n",
    "    tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(units=64, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=1, activation='relu')\n",
    "])\n",
    "\n",
    "model_weight.compile(loss='mse', optimizer='adam', metrics=['mae'])\n",
    "\n",
    "model_weight.fit(X_train, np.log(y_weight_train), epochs=13, batch_size=1)\n",
    "\n",
    "test_loss_weight_cnn, test_weight_accuracy = model_weight.evaluate(X_test, np.log(y_weight_test))\n",
    "\n",
    "logging.info(\"model weight loss : %f, mae : %f\", test_loss_weight_cnn,test_weight_accuracy)\n",
    "print('loss of weight:', test_loss_weight_cnn)\n",
    "print('mae of weight:', test_weight_accuracy)\n",
    "\n",
    "#saving the model\n",
    "model_weight.save('weight_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "OGsejAFARJNp"
   },
   "outputs": [],
   "source": [
    "#load the models\n",
    "\n",
    "height_model = tf.keras.models.load_model('height_model.h5')\n",
    "weight_model = tf.keras.models.load_model('weight_model.h5')\n",
    "\n",
    "\n",
    "def predict_height_weight_BMI(input_img,height_model,weight_model):\n",
    "    logging.info(\"Predicting height, weight, and BMI for image %s\", input_img)\n",
    "    start_time = time.time()\n",
    "    test_array = np.expand_dims(np.array(my_face_encoding(input_img)),axis=0)\n",
    "    height = np.ndarray.item(np.exp(height_model.predict(test_array)))\n",
    "    weight = np.ndarray.item(np.exp(weight_model.predict(test_array)))\n",
    "    bmi = weight / (height)**2\n",
    "    end_time = time.time()\n",
    "    runtime = end_time - start_time\n",
    "    logging.info(\"Predicted height: %f, weight: %f, BMI: %f, runtime : %s\", height, weight, bmi, runtime)\n",
    "    return {'height':height,\"weight\":weight,\"bmi\":bmi,'runtime':runtime}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JRZVeViRRJNq"
   },
   "source": [
    "for code level deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "KXh5Zg_gRJNr"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 347
    },
    "id": "hcz9nSQJRJNs",
    "outputId": "15138acf-a003-4671-f52d-d73aabd3e192"
   },
   "outputs": [
    {
     "data": {
      "image/jpeg": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_img = 'sample_faces/akshay1.jpeg'\n",
    "Image(input_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "45rIX_9xRJNu",
    "outputId": "1d3bbb48-9b96-49df-95e8-54a5eaccb470"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_faces/akshay1.jpeg\n",
      "1/1 [==============================] - 0s 172ms/step\n",
      "1/1 [==============================] - 0s 141ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'height': 1.7767481803894043,\n",
       " 'weight': 79.6153793334961,\n",
       " 'bmi': 25.22000741861471,\n",
       " 'runtime': 1.0312461853027344}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_height_weight_BMI(input_img,height_model,weight_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO L460\\AppData\\Local\\Temp\\ipykernel_11112\\889180239.py:66: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "  background_image = background_image.resize((400, 500), Image.LANCZOS)\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "from PIL import ImageTk, Image\n",
    "import cv2\n",
    "\n",
    "# Load the height, weight, and BMI models\n",
    "height_model = tf.keras.models.load_model('height_model.h5')\n",
    "weight_model = tf.keras.models.load_model('weight_model.h5')\n",
    "\n",
    "# Define the function to predict height, weight, and BMI\n",
    "def predict_height_weight_BMI(input_img, height_model, weight_model):\n",
    "    logging.info(\"Predicting height, weight, and BMI for image %s\", input_img)\n",
    "    test_array = np.expand_dims(np.array(my_face_encoding(input_img)),axis=0)\n",
    "    height = np.ndarray.item(np.exp(height_model.predict(test_array)))\n",
    "    weight = np.ndarray.item(np.exp(weight_model.predict(test_array)))\n",
    "    bmi = weight / (height)**2\n",
    "    logging.info(\"Predicted height: %f, weight: %f, BMI: %f\", height, weight, bmi)\n",
    "    return {'height':height, \"weight\":weight, \"bmi\":bmi}\n",
    "\n",
    "def browse_file():\n",
    "    file_path = filedialog.askopenfilename()\n",
    "    if file_path:\n",
    "        image = Image.open(file_path)\n",
    "        image = image.resize((300, 300), Image.LANCZOS)\n",
    "        photo = ImageTk.PhotoImage(image)\n",
    "        canvas.image = photo  # Save a reference to the image to prevent it from being garbage collected\n",
    "        image_id = canvas.create_image(0, 0, anchor='nw', image=photo)  # Create image_id when loading a new image\n",
    "        result = predict_height_weight_BMI(file_path, height_model, weight_model)\n",
    "        result_label.config(text=f\"Height: {result['height']:.2f}\\nWeight: {result['weight']:.2f}\\nBMI: {result['bmi']:.2f}\")\n",
    "\n",
    "def capture_image():\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    cap.set(3, 640)\n",
    "    cap.set(4, 480)\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            cv2.imshow('frame', frame)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                cv2.imwrite('captured_image.jpg', frame)\n",
    "                new_image = Image.open('captured_image.jpg')\n",
    "                new_image = new_image.resize((300, 300), Image.LANCZOS)\n",
    "                photo = ImageTk.PhotoImage(new_image)\n",
    "                canvas.image = photo  # Save a reference to the image\n",
    "                image_id = canvas.create_image(0, 0, anchor='nw', image=photo)  # Create image_id when loading a new image\n",
    "                result = predict_height_weight_BMI('captured_image.jpg', height_model, weight_model)\n",
    "                result_label.config(text=f\"Height: {result['height']:.2f}\\nWeight: {result['weight']:.2f}\\nBMI: {result['bmi']:.2f}\")\n",
    "                break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "def clear_display():\n",
    "    canvas.delete(image_id)  # Delete the image from the canvas\n",
    "    canvas.image = None  # Remove the reference to the current image\n",
    "    result_label.config(text=\"\")  # Clear the result text\n",
    "\n",
    "\n",
    "root = tk.Tk()\n",
    "root.title(\"Body Mass Index from Face Images\")\n",
    "root.iconbitmap(r'C:\\Users\\LENOVO L460\\Downloads\\Body-Mass-Index-from-Face-Images-main\\Body-Mass-Index-from-Face-Images-main\\bmi.ico')\n",
    "\n",
    "\n",
    "background_image = Image.open(\"Capture.PNG\")\n",
    "background_image = background_image.resize((400, 500), Image.LANCZOS)\n",
    "background_image = ImageTk.PhotoImage(background_image)\n",
    "background_label = tk.Label(root, image=background_image)\n",
    "background_label.place(x=0, y=0, relwidth=1, relheight=1)\n",
    "\n",
    "canvas_frame = tk.Frame(root, width=300, height=300)\n",
    "canvas_frame.pack(pady=20)\n",
    "\n",
    "canvas = tk.Canvas(canvas_frame, width=300, height=300)\n",
    "canvas.pack()\n",
    "\n",
    "result_frame = tk.Frame(root)\n",
    "result_frame.pack(pady=10)\n",
    "\n",
    "result_label = tk.Label(result_frame, font=(\"Arial\", 14), bg='white')\n",
    "result_label.pack()\n",
    "\n",
    "button_frame = tk.Frame(root)\n",
    "button_frame.pack(pady=10)\n",
    "\n",
    "browse_button = tk.Button(button_frame, text=\"Browse the Image\", command=browse_file, bg='skyblue', fg='black', relief='solid')\n",
    "browse_button.pack(side=tk.LEFT, padx=10)\n",
    "\n",
    "capture_button = tk.Button(button_frame, text=\"Capture Using Webcam\", command=capture_image, bg='pink', fg='black', relief='solid')\n",
    "capture_button.pack(side=tk.LEFT, padx=10)\n",
    "\n",
    "clear_button = tk.Button(button_frame, text=\"Clear the Result\", command=clear_display, bg='orange', fg='black', relief='solid')\n",
    "clear_button.pack(side=tk.LEFT, padx=10)\n",
    "\n",
    "image_id = canvas.create_image(0, 0, anchor='nw')\n",
    "\n",
    "root.geometry(\"400x500\")\n",
    "root.resizable(False, False)  # Disable resizing\n",
    "root.mainloop()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "from PIL import ImageTk, Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "\n",
    "# Load the height, weight, and BMI models\n",
    "height_model = tf.keras.models.load_model('height_model.h5')\n",
    "weight_model = tf.keras.models.load_model('weight_model.h5')\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Define the function to predict height, weight, and BMI\n",
    "def predict_height_weight_BMI(input_img, height_model, weight_model):\n",
    "    logging.info(\"Predicting height, weight, and BMI for image %s\", input_img)\n",
    "    test_array = np.expand_dims(np.array(my_face_encoding(input_img)), axis=0)\n",
    "    height = np.ndarray.item(np.exp(height_model.predict(test_array)))\n",
    "    weight = np.ndarray.item(np.exp(weight_model.predict(test_array)))\n",
    "    bmi = weight / (height)**2\n",
    "    logging.info(\"Predicted height: %f, weight: %f, BMI: %f\", height, weight, bmi)\n",
    "    return {'height': height, \"weight\": weight, \"bmi\": bmi}\n",
    "\n",
    "def browse_file():\n",
    "    file_path = filedialog.askopenfilename()\n",
    "    if file_path:\n",
    "        image = Image.open(file_path)\n",
    "        image = image.resize((300, 300), Image.LANCZOS)\n",
    "        photo = ImageTk.PhotoImage(image)\n",
    "        canvas.image = photo  # Save a reference to the image\n",
    "        canvas.create_image(0, 0, anchor='nw', image=photo)  # Create image_id when loading a new image\n",
    "        result = predict_height_weight_BMI(file_path, height_model, weight_model)\n",
    "        result_label.config(text=f\"Height: {result['height']:.2f}\\nWeight: {result['weight']:.2f}\\nBMI: {result['bmi']:.2f}\")\n",
    "\n",
    "def capture_image():\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    cap.set(3, 640)\n",
    "    cap.set(4, 480)\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            cv2.imshow('frame', frame)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                cv2.imwrite('captured_image.jpg', frame)\n",
    "                new_image = Image.open('captured_image.jpg')\n",
    "                new_image = new_image.resize((300, 300), Image.LANCZOS)\n",
    "                photo = ImageTk.PhotoImage(new_image)\n",
    "                canvas.image = photo  # Save a reference to the image\n",
    "                canvas.create_image(0, 0, anchor='nw', image=photo)  # Create image_id when loading a new image\n",
    "                result = predict_height_weight_BMI('captured_image.jpg', height_model, weight_model)\n",
    "                result_label.config(text=f\"Height: {result['height']:.2f}\\nWeight: {result['weight']:.2f}\\nBMI: {result['bmi']:.2f}\")\n",
    "                break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "def clear_display():\n",
    "    canvas.delete(image_id)  # Delete the image from the canvas\n",
    "    canvas.image = None  # Remove the reference to the current image\n",
    "    result_label.config(text=\"\")  # Clear the result text\n",
    "\n",
    "# Create Tkinter GUI\n",
    "root = tk.Tk()\n",
    "root.title(\"Body Mass Index from Face Images\")\n",
    "\n",
    "# Add GUI components\n",
    "background_image = Image.open(\"Capture.PNG\")\n",
    "background_image = background_image.resize((400, 500), Image.LANCZOS)\n",
    "background_image = ImageTk.PhotoImage(background_image)\n",
    "background_label = tk.Label(root, image=background_image)\n",
    "background_label.place(x=0, y=0, relwidth=1, relheight=1)\n",
    "\n",
    "canvas_frame = tk.Frame(root, width=300, height=300)\n",
    "canvas_frame.pack(pady=20)\n",
    "\n",
    "canvas = tk.Canvas(canvas_frame, width=300, height=300)\n",
    "canvas.pack()\n",
    "\n",
    "result_frame = tk.Frame(root)\n",
    "result_frame.pack(pady=10)\n",
    "\n",
    "result_label = tk.Label(result_frame, font=(\"Arial\", 14), bg='white')\n",
    "result_label.pack()\n",
    "\n",
    "button_frame = tk.Frame(root)\n",
    "button_frame.pack(pady=10)\n",
    "\n",
    "browse_button = tk.Button(button_frame, text=\"Browse the Image\", command=browse_file, bg='skyblue', fg='black', relief='solid')\n",
    "browse_button.pack(side=tk.LEFT, padx=10)\n",
    "\n",
    "capture_button = tk.Button(button_frame, text=\"Capture Using Webcam\", command=capture_image, bg='pink', fg='black', relief='solid')\n",
    "capture_button.pack(side=tk.LEFT, padx=10)\n",
    "\n",
    "clear_button = tk.Button(button_frame, text=\"Clear the Result\", command=clear_display, bg='orange', fg='black', relief='solid')\n",
    "clear_button.pack(side=tk.LEFT, padx=10)\n",
    "\n",
    "image_id = canvas.create_image(0, 0, anchor='nw')\n",
    "\n",
    "root.geometry(\"400x500\")\n",
    "root.resizable(False, False)  # Disable resizing\n",
    "\n",
    "# Start the Tkinter event loop\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "from PIL import ImageTk, Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "\n",
    "# Load the height, weight, and BMI models\n",
    "height_model = tf.keras.models.load_model('height_model.h5')\n",
    "weight_model = tf.keras.models.load_model('weight_model.h5')\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Define the function to predict height, weight, and BMI\n",
    "def predict_height_weight_BMI(input_img, height_model, weight_model):\n",
    "    logging.info(\"Predicting height, weight, and BMI for image %s\", input_img)\n",
    "    test_array = np.expand_dims(np.array(my_face_encoding(input_img)), axis=0)\n",
    "    height = np.ndarray.item(np.exp(height_model.predict(test_array)))\n",
    "    weight = np.ndarray.item(np.exp(weight_model.predict(test_array)))\n",
    "    bmi = weight / (height)**2\n",
    "    logging.info(\"Predicted height: %f, weight: %f, BMI: %f\", height, weight, bmi)\n",
    "    return {'height': height, \"weight\": weight, \"bmi\": bmi}\n",
    "\n",
    "def browse_file():\n",
    "    file_path = filedialog.askopenfilename()\n",
    "    if file_path:\n",
    "        image = Image.open(file_path)\n",
    "        image = image.resize((300, 300), Image.LANCZOS)\n",
    "        photo = ImageTk.PhotoImage(image)\n",
    "        canvas.image = photo  # Save a reference to the image\n",
    "        canvas.create_image(0, 0, anchor='nw', image=photo)  # Create image_id when loading a new image\n",
    "        result = predict_height_weight_BMI(file_path, height_model, weight_model)\n",
    "        result_label.config(text=f\"Height: {result['height']:.2f}\\nWeight: {result['weight']:.2f}\\nBMI: {result['bmi']:.2f}\")\n",
    "\n",
    "def capture_image():\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    cap.set(3, 640)\n",
    "    cap.set(4, 480)\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            cv2.imshow('frame', frame)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                cv2.imwrite('captured_image.jpg', frame)\n",
    "                new_image = Image.open('captured_image.jpg')\n",
    "                new_image = new_image.resize((300, 300), Image.LANCZOS)\n",
    "                photo = ImageTk.PhotoImage(new_image)\n",
    "                canvas.image = photo  # Save a reference to the image\n",
    "                canvas.create_image(0, 0, anchor='nw', image=photo)  # Create image_id when loading a new image\n",
    "                result = predict_height_weight_BMI('captured_image.jpg', height_model, weight_model)\n",
    "                result_label.config(text=f\"Height: {result['height']:.2f}\\nWeight: {result['weight']:.2f}\\nBMI: {result['bmi']:.2f}\")\n",
    "                break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "def clear_display():\n",
    "    canvas.delete(\"all\")  # Delete all items from the canvas\n",
    "    canvas.image = None  # Remove the reference to the current image\n",
    "    result_label.config(text=\"\")  # Clear the result text\n",
    "\n",
    "# Create Tkinter GUI\n",
    "root = tk.Tk()\n",
    "root.title(\"Body Mass Index from Face Images\")\n",
    "\n",
    "# Add GUI components\n",
    "background_image = Image.open(\"Capture.PNG\")\n",
    "background_image = background_image.resize((400, 500), Image.LANCZOS)\n",
    "background_image = ImageTk.PhotoImage(background_image)\n",
    "background_label = tk.Label(root, image=background_image)\n",
    "background_label.image = background_image  # Save a reference to the image\n",
    "background_label.place(x=0, y=0, relwidth=1, relheight=1)\n",
    "\n",
    "canvas_frame = tk.Frame(root, width=300, height=300)\n",
    "canvas_frame.pack(pady=20)\n",
    "\n",
    "canvas = tk.Canvas(canvas_frame, width=300, height=300)\n",
    "canvas.pack()\n",
    "\n",
    "result_frame = tk.Frame(root)\n",
    "result_frame.pack(pady=10)\n",
    "\n",
    "result_label = tk.Label(result_frame, font=(\"Arial\", 14), bg='white')\n",
    "result_label.pack()\n",
    "\n",
    "button_frame = tk.Frame(root)\n",
    "button_frame.pack(pady=10)\n",
    "\n",
    "browse_button = tk.Button(button_frame, text=\"Browse the Image\", command=browse_file, bg='skyblue', fg='black', relief='solid')\n",
    "browse_button.pack(side=tk.LEFT, padx=10)\n",
    "\n",
    "capture_button = tk.Button(button_frame, text=\"Capture Using Webcam\", command=capture_image, bg='pink', fg='black', relief='solid')\n",
    "capture_button.pack(side=tk.LEFT, padx=10)\n",
    "\n",
    "clear_button = tk.Button(button_frame, text=\"Clear the Result\", command=clear_display, bg='orange', fg='black', relief='solid')\n",
    "clear_button.pack(side=tk.LEFT, padx=10)\n",
    "\n",
    "root.geometry(\"400x500\")\n",
    "root.resizable(False, False)  # Disable resizing\n",
    "\n",
    "# Start the Tkinter event loop\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "from PIL import ImageTk, Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "\n",
    "# Load the height, weight, and BMI models\n",
    "height_model = tf.keras.models.load_model('height_model.h5')\n",
    "weight_model = tf.keras.models.load_model('weight_model.h5')\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Define the function to predict height, weight, and BMI\n",
    "def predict_height_weight_BMI(input_img, height_model, weight_model):\n",
    "    logging.info(\"Predicting height, weight, and BMI for image %s\", input_img)\n",
    "    test_array = np.expand_dims(np.array(my_face_encoding(input_img)), axis=0)\n",
    "    height = np.ndarray.item(np.exp(height_model.predict(test_array)))\n",
    "    height_feet = height * 3.28084  # Convert height to feet\n",
    "    weight = np.ndarray.item(np.exp(weight_model.predict(test_array)))\n",
    "    \n",
    "    bmi = weight / (height)**2\n",
    "    logging.info(\"Predicted height: %f meters (%f feet), weight: %f, BMI: %f\", height, weight, bmi)\n",
    "    return {'height': height_feet, \"weight\": weight, \"bmi\": bmi}\n",
    "\n",
    "def browse_file():\n",
    "    file_path = filedialog.askopenfilename()\n",
    "    if file_path:\n",
    "        image = Image.open(file_path)\n",
    "        image = image.resize((300, 300), Image.LANCZOS)\n",
    "        photo = ImageTk.PhotoImage(image)\n",
    "        background_label.configure(image=photo)\n",
    "        background_label.image = photo  # Save a reference to the image\n",
    "        result = predict_height_weight_BMI(file_path, height_model, weight_model)\n",
    "        result_label.config(text=f\"Height: {result['height']:.2f}\\nWeight: {result['weight']:.2f}\\nBMI: {result['bmi']:.2f}\")\n",
    "\n",
    "def capture_image():\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    cap.set(3, 640)\n",
    "    cap.set(4, 480)\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            cv2.imshow('frame', frame)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                cv2.imwrite('captured_image.jpg', frame)\n",
    "                new_image = Image.open('captured_image.jpg')\n",
    "                new_image = new_image.resize((300, 300), Image.LANCZOS)\n",
    "                photo = ImageTk.PhotoImage(new_image)\n",
    "                background_label.configure(image=photo)\n",
    "                background_label.image = photo  # Save a reference to the image\n",
    "                result = predict_height_weight_BMI('captured_image.jpg', height_model, weight_model)\n",
    "                result_label.config(text=f\"Height: {result['height']:.2f}\\nWeight: {result['weight']:.2f}\\nBMI: {result['bmi']:.2f}\")\n",
    "                break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "def clear_display():\n",
    "    background_label.config(image='')  # Clear the background image\n",
    "    result_label.config(text=\"\")  # Clear the result text\n",
    "\n",
    "# Create Tkinter GUI\n",
    "root = tk.Tk()\n",
    "root.title(\"Body Mass Index from Face Images\")\n",
    "\n",
    "# Add GUI components\n",
    "background_image = Image.open(\"Capture.PNG\")\n",
    "background_image = background_image.resize((400, 500), Image.LANCZOS)\n",
    "background_image = ImageTk.PhotoImage(background_image)\n",
    "background_label = tk.Label(root, image=background_image)\n",
    "background_label.image = background_image  # Save a reference to the image\n",
    "background_label.place(x=0, y=0, relwidth=1, relheight=1)\n",
    "\n",
    "canvas_frame = tk.Frame(root, width=300, height=300)\n",
    "canvas_frame.pack(pady=20)\n",
    "\n",
    "result_frame = tk.Frame(root)\n",
    "result_frame.pack(pady=10)\n",
    "\n",
    "result_label = tk.Label(result_frame, font=(\"Arial\", 14), bg='white')\n",
    "result_label.pack()\n",
    "\n",
    "button_frame = tk.Frame(root)\n",
    "button_frame.pack(pady=10)\n",
    "\n",
    "browse_button = tk.Button(button_frame, text=\"Browse the Image\", command=browse_file, bg='skyblue', fg='black', relief='solid')\n",
    "browse_button.pack(side=tk.LEFT, padx=10)\n",
    "\n",
    "capture_button = tk.Button(button_frame, text=\"Capture Using Webcam\", command=capture_image, bg='pink', fg='black', relief='solid')\n",
    "capture_button.pack(side=tk.LEFT, padx=10)\n",
    "\n",
    "clear_button = tk.Button(button_frame, text=\"Clear the Result\", command=clear_display, bg='orange', fg='black', relief='solid')\n",
    "clear_button.pack(side=tk.LEFT, padx=10)\n",
    "\n",
    "root.geometry(\"400x500\")\n",
    "root.resizable(False, False)  # Disable resizing\n",
    "\n",
    "# Start the Tkinter event loop\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "e3420ed2c65300ffc1d71199033e9176e0c26ff8ced7474b2f05f6e440a26153"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
